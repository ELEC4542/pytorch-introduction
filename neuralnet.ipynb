{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "\n",
    "In this notebook, we revisit what is a Neural Network, and how to train a Neural Network with PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will also be using the `torchvision` module as it provides additional functionalities in Computer Vision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import optim, nn, Tensor\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "from __future__ import annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "      super(Net, self).__init__()\n",
    "\n",
    "      # First 2D convolutional layer, taking in 3 input channel (RGB image),\n",
    "      # outputting 32 convolutional features, with a square kernel size of 3\n",
    "      self.conv1 = nn.Conv2d(3, 32, 3, 1)\n",
    "      # Second 2D convolutional layer, taking in the 32 input layers,\n",
    "      # outputting 64 convolutional features, with a square kernel size of 3\n",
    "      self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "\n",
    "      # Designed to ensure that adjacent pixels are either all 0s or all active\n",
    "      # with an input probability\n",
    "      self.dropout1 = nn.Dropout2d(0.25)\n",
    "      self.dropout2 = nn.Dropout2d(0.5)\n",
    "\n",
    "      # First fully connected layer\n",
    "      self.fc1 = nn.Linear(12544, 1024)\n",
    "      # Second fully connected layer that outputs our 100 labels\n",
    "      self.fc2 = nn.Linear(1024, 100)\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "      # Pass data through conv1\n",
    "      x = self.conv1(x)\n",
    "      # Use the rectified-linear activation function over x\n",
    "      x = F.relu(x)\n",
    "\n",
    "      x = self.conv2(x)\n",
    "      x = F.relu(x)\n",
    "\n",
    "      # Run max pooling over x\n",
    "      x = F.max_pool2d(x, 2)\n",
    "      # Pass data through dropout1\n",
    "      x = self.dropout1(x)\n",
    "      # Flatten x with start_dim=1\n",
    "      x = torch.flatten(x, 1)\n",
    "      # Pass data through ``fc1``\n",
    "      x = self.fc1(x)\n",
    "      x = F.relu(x)\n",
    "      x = self.dropout2(x)\n",
    "      x = self.fc2(x)\n",
    "\n",
    "      # Apply softmax to x\n",
    "      # `softmax` opertion is often not needed as Criterion has it built-in\n",
    "      # x = F.log_softmax(x, dim=1)\n",
    "      return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build `Dataset` and `DataLoader`\n",
    "\n",
    "`torch.utils.data` provides 3 utilities classes to help you manage your data.\n",
    "\n",
    "1. `Dataset`: read data from disk & performs data augmentation\n",
    "2. `Sampler`: sample a batch of indices from the dataset (optional, `DataLoader` has default `Sampler`)\n",
    "3. `DataLoader`: collate multiple data points to a batch (use built-in `DataLoader` in most cases)\n",
    "\n",
    "In many cases, you will need to write your own `Dataset` to read your own data.\n",
    "\n",
    "`torchvision` also has some pre-defined dataset classes like `torchvision.datasets.ImageFolder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSet(Dataset):\n",
    "    def __init__(self):\n",
    "        ...\n",
    "    def __getitem__(self, index: int):\n",
    "        ...\n",
    "    def __getitems__(self, indices: list[int]):\n",
    "        ...\n",
    "    def __len__(self):\n",
    "        ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/transforms/v2/_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to cifar100/cifar-100-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169001437/169001437 [00:14<00:00, 11287195.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting cifar100/cifar-100-python.tar.gz to cifar100\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = v2.Compose(\n",
    "    [\n",
    "        # Convert image read from the disk (in PIL.Image or OpenCV) to Tensor\n",
    "        v2.ToTensor(),\n",
    "        # Normalize the image to a normal distribution (with a mean of 0 and a standard deviation of 1)\n",
    "        # note thad different dataset may have their own `mean` and `std`\n",
    "        v2.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "        # Optional data augmentations \n",
    "        v2.RandomResizedCrop((32, 32), antialias=True),\n",
    "        v2.RandomVerticalFlip(),\n",
    "        v2.RandomHorizontalFlip(),\n",
    "        v2.ColorJitter(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_data = datasets.CIFAR100(\"cifar100\", transform=transform, download=True)\n",
    "val_data = datasets.CIFAR100(\"cifar100\", transform=transform, train=False, download=True)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:1374: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 4.599312782287598\n",
      "Loss: 4.631166934967041\n",
      "Loss: 4.621702671051025\n",
      "Loss: 4.526247024536133\n",
      "Loss: 4.630471229553223\n",
      "Loss: 4.610811710357666\n",
      "Loss: 4.499372482299805\n",
      "Loss: 4.599653720855713\n",
      "Loss: 4.548818588256836\n",
      "Loss: 4.4871826171875\n",
      "Loss: 4.45155143737793\n",
      "Loss: 4.355360984802246\n",
      "Loss: 3.9637246131896973\n",
      "Loss: 4.619658470153809\n",
      "Loss: 4.4346771240234375\n",
      "Loss: 4.777684211730957\n",
      "Loss: 4.345715045928955\n",
      "Loss: 3.9502010345458984\n",
      "Loss: 4.4586029052734375\n",
      "Loss: 4.400957107543945\n",
      "Loss: 4.498456954956055\n",
      "Loss: 4.248757362365723\n",
      "Loss: 4.136300563812256\n",
      "Loss: 4.589773654937744\n",
      "Loss: 4.244424343109131\n",
      "Loss: 4.136141300201416\n",
      "Loss: 4.372920513153076\n",
      "Loss: 4.847926139831543\n",
      "Loss: 4.069480895996094\n",
      "Loss: 3.8857955932617188\n",
      "Loss: 4.184459686279297\n",
      "Loss: 4.309844493865967\n",
      "Loss: 4.668144226074219\n",
      "Loss: 4.744712829589844\n",
      "Loss: 3.9085421562194824\n",
      "Loss: 4.07913875579834\n",
      "Loss: 3.8500194549560547\n",
      "Loss: 4.152214527130127\n",
      "Loss: 4.617140769958496\n",
      "Loss: 4.724593162536621\n",
      "Loss: 4.0057597160339355\n",
      "Loss: 4.1569504737854\n",
      "Loss: 4.143741130828857\n",
      "Loss: 4.935110569000244\n",
      "Loss: 4.0700788497924805\n",
      "Loss: 3.914372444152832\n",
      "Loss: 3.710529327392578\n",
      "Loss: 4.391177654266357\n",
      "Loss: 4.292298316955566\n",
      "Loss: 4.155435562133789\n",
      "Loss: 4.569930553436279\n",
      "Loss: 4.297791004180908\n",
      "Loss: 4.165136337280273\n",
      "Loss: 4.355897426605225\n",
      "Loss: 3.8671271800994873\n",
      "Loss: 4.253604412078857\n",
      "Loss: 4.506066799163818\n",
      "Loss: 4.02325439453125\n",
      "Loss: 4.318628787994385\n",
      "Loss: 4.201709270477295\n",
      "Loss: 4.266872406005859\n",
      "Loss: 3.6652419567108154\n",
      "Loss: 4.275063514709473\n"
     ]
    }
   ],
   "source": [
    "def train_step(input: Tensor, label: Tensor):\n",
    "    output = model(input)\n",
    "    loss = criterion(output, label)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    scheduler.step()\n",
    "    return loss, output\n",
    "\n",
    "def train_epoch(dataloader):\n",
    "    for i, (input, label) in enumerate(dataloader):\n",
    "        loss, output = train_step(input, label)\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Loss: {loss.item()}\")\n",
    "\n",
    "train_epoch(train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 4.295488357543945, Accuracy: 0.0\n",
      "Loss: 4.139092445373535, Accuracy: 0.0\n",
      "Loss: 4.592042922973633, Accuracy: 0.0\n",
      "Loss: 3.7268476486206055, Accuracy: 0.125\n",
      "Loss: 3.7312283515930176, Accuracy: 0.0\n",
      "Loss: 4.1175856590271, Accuracy: 0.125\n",
      "Loss: 4.0747809410095215, Accuracy: 0.0\n",
      "Loss: 4.104453086853027, Accuracy: 0.0\n",
      "Loss: 3.6606931686401367, Accuracy: 0.125\n",
      "Loss: 4.5535569190979, Accuracy: 0.0\n",
      "Loss: 4.485076904296875, Accuracy: 0.0\n",
      "Loss: 4.329070091247559, Accuracy: 0.0\n",
      "Loss: 4.460677623748779, Accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_step(input, label):\n",
    "    output = model(input)\n",
    "    loss = criterion(output, label)\n",
    "    return loss, output\n",
    "\n",
    "def evaluate_epoch(dataloader):\n",
    "    for i, (input, label) in enumerate(dataloader):\n",
    "        loss, output = evaluate_step(input, label)\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        accuracy = pred.eq(label.view_as(pred)).sum().item() / len(label)\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Loss: {loss.item()}, Accuracy: {accuracy}\")\n",
    "\n",
    "evaluate_epoch(val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save your trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
